---
title: Python爬虫
description: 爬虫教程
published: 1
date: 2022-10-23T09:31:59.556Z
tags: 爬虫
editor: markdown
dateCreated: 2022-10-13T13:23:27.722Z
---

# request入门
+ 简介
requests专门用于发送http请求，可以模拟页面的http请求，下面主要介绍以下内容：
（1）发送http请求
（2）设置http请求头
（3）抓取二进制数据
（4）post请求
（5）响应数据
（6）上传文件
（7）处理Cookie
（8）维持会话
（9）ssl证书验证
（10）使用代理
（11）超时处理
（12）身份验证
（13）打包请求
+ 环境安装
执行安装命令
```  shell
pip install requests
```
# 发送http请求
## 基本用法
+ 介绍
使用requests的get方法访问淘宝首页并返回get方法返回值类型、状态码、响应体、Cookie等信息。
+ 代码
``` py
import requests
r = requests.get('https://www.taobao.com')
print(type(r))
print(r.status_code)
print(type(r.text))
print(r.cookies)
print(r.text)
```
## GET请求
+ 描述
（1）get方法带参数有两种方式：
方式一：在url后面用？分隔参数
方式二：使用get方法的params参数将需要的参数以字典的方式存储
（2）两种方式可以共存：
当两种方式共存时会将参数合并，若出现同名参数，会用列表存储，同名参数的值会按先后顺序存放在列表中
+ 代码
``` py
import requests
data = {
    'name':'Bill',
    'country':'中国',
    'age':20
}

r = requests.get('http://httpbin.org/get?name=Mike&country=美国&age=40',params=data)
print(r.text)
print(r.json())
print(r.json()['args']['country'])
```
# 设置http请求头
+ 描述
1. 为get方法添加http请求头需设置get方法的headers参数即可。
2. 该参数是一个字典类型的值，每一对key-value就是一个Cookie，如果设置中文的cookie需要使用quote和unquote进行编码解码。
+ 代码
``` py
import requests
from urllib.parse import quote,unquote

headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',
    'name':quote('李宁')
}

r = requests.get('http://httpbin.org/get',headers=headers)
print(r.text)
print('Name:',unquote(r.json()['headers']['Name']))
```
# 抓取二进制数据
+ 描述
获取二进制数据如png图片、PDF文档等，需调用Response.content属性获取bytes形式的数据，然后用相应的api保存在文件中
+ 代码
用get方法抓取一个png格式的图像文件，并将其保存为本地文件
``` py
import requests

r = requests.get('http://t.cn/EfgN7gz')
print(r.text)
with open('Python从菜鸟到高手.png','wb') as f:
    f.write(r.content)
```
# post请求
+ 描述
发送post请求时需要指定data参数，改参数是字典类型的值，每一对key-value是一对post请求参数。
+ 代码
``` py
import requests
data = {
    'name':'Bill',
    'country':'中国',
    'age':20
}

r = requests.post('http://httpbin.org/post',data=data)
print(r.text)
print(r.json())
print(r.json()['form']['country'])
```
# 响应数据
+ 描述
发送http请求get和post方法会返回响应对象（Response对象），可使用text属性和content属性获得响应内容，还可以使用其他属性或者方法获得状态码、响应头、Cookie等。
+ 代码
``` py
import requests

headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',
}

r = requests.get('http://www.jianshu.com',headers=headers)
print(type(r.status_code),r.status_code)
print(type(r.headers),r.headers)
print(type(r.cookies),r.cookies)
print(type(r.url),r.url)
print(type(r.history),r.history)

if not r.status_code == requests.codes.okay:
    print("failed")
else:
    print("ok")
```
# 上传文件
+ 描述
用requests上传文件需指定post方法的files参数，files参数的值为BufferedReader对象，该对象可以用Python语言的内置函数open返回。
+ 代码
``` py
import requests

files1 = {'file':open('Python从菜鸟到高手.png','rb')}
r1 = requests.post('http://127.0.0.1:5000', files=files1)
print(r1.text)
files2 = {'file':open('Python从菜鸟到高手.png','rb')}
r2 = requests.post('http://httpbin.org/post',files=files2)
print(r2.text)
```
# 处理cookie
+ 描述
通过requests的cookies属性可以得到服务器发送过来的Cookie，设置Cookie有两种方法：
（1）headers参数
（2）cookies参数
get和post都有这两个参数，如果使用cookies参数需要创建RequestsCookieJar对象，并使用set方法设置每一个Cookie
+ 代码
``` py
import requests
r1 = requests.get('http://www.baidu.com')
print(r1.cookies)
for key,value in r1.cookies.items():
    print(key,'=',value)

# 获取简书首页内容
headers = {
    'Host':'www.jianshu.com',
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',
    'Cookie':'Hm_lvt_0c0e9d9b1e7d617b3e6842e85b9fb068=1550805089,1550815557,155116976; locale=zh-CN; read_mode=day; default_font=font2; remember_user_token=W1sxMzYxNDI1OF0sIiQyYSQxMSRWWDhUU0JKOU5oZDZtYjhoblMwclYuIiwiMTU1MTM0Nzk5MS4wMzU3MjI3Il0%3D--04787a1b6cfda5ed5974bf50e178b899e99eb4ec; __yadk_uid=xeqG3EJDiKBRfxVO3j2WeLKEUSNMutrB; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2213614258%22%2C%22%24device_id%22%3A%22169338b879465f-05e36cc0827c6-36657105-3686400-169338b8795759%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%221699465f-05e36cc0827c6-36657105-3686400-169338b8795759%22%7D; _m7e_session_core=f79a4a7802e2ffb7035adf0c44294875; Hm_lpvt_0c0e9d9b1e7d617b3e6842e85b9fb068=1551429715'

}

r2 = requests.get('https://www.jianshu.com',headers=headers)
print(r2.text)

# 另外一种设置Cookie的方式
headers = {
    'Host':'www.jianshu.com',
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',
}
cookies = 'Hm_lvt_0c0e9d9b1e7d617b3e6842e85b9fb068=1550805089,1550815557,1551167360,1551347976; locale=zh-CN; read_mode=day; default_font=font2; remember_user_token=W1sxMzYxNDI1OF0sIiQyYSQxMSRWWDhUU0JKOU5oZDZtYjhoblMwclYuIiwiMTU1MTM0Nzk5MS4wMzU3MjI3Il0%3D--04787a1b6cfda5ed5974bf50e178b899e99eb4ec; __yadk_uid=xeqG3EJDiKBRfxVO3j2WeLKEUSNMutrB; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2213614258%22%2C%22%24device_id%22%3A%22169338b879465f-05e36cc0827c6-36657105-3686400-169338b8795759%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%22169338b879465f-05e36cc0827c6-36657105-3686400-169338b8795759%22%7D; _m7e_session_core=f79a4a7802e2ffb7035adf0c44294875; Hm_lpvt_0c0e9d9b1e7d617b3e6842e85b9fb068=1551429715'
jar = requests.cookies.RequestsCookieJar()
for cookie in cookies.split(';'):
    key, value = cookie.split('=',1)
    jar.set(key,value)
r3 = requests.get('http://www.jianshu.com',cookies = jar,headers=headers)
print(r3.text)
```
# 使用同一个会话
+ 描述
客户端和服务端来回传递对应ID,通过它客户单可以在服务端找到对应的Session对象。一些爬虫需要作为同一个客户端来多次抓取页面，
也就是抓取动作需要在同一个Session中完成。最直接的方法是不断向服务端发送同一个ID即Cookie但是需要自己操作。在requests中为我们提供了Session对象自动维持Session,通过Session对象的get、post方法可以在同一个Session中向服务端发送请求。
+ 代码
``` py
import requests
# 不使用Session
requests.get('http://httpbin.org/cookies/set/name/Bill')
r1 = requests.get('http://httpbin.org/cookies')
print(r1.text)

# 使用Session
session = requests.Session()
session.get('http://httpbin.org/cookies/set/name/Bill')
r2 = session.get('http://httpbin.org/cookies')
print(r2.text)
```
# SSL证书验证
使用verify参数可以将ssl证书验证功能关掉，verify参数的默认值是True表示验证证书，如果将verify参数设为false表示关掉验证功能，显示的警告客户以使用urllib3的disable_warnings函数禁止显示的警告信息。
+ 代码
``` py
from requests import exceptions,get
import urllib3
try:
    response = get('https://localhost')
    print(response.status_code)
except exceptions.SSLError as e:
    print(e.args[0])


try:
    response = get('https://localhost',verify=False)
    print(response.status_code)
except exceptions.SSLError as e:
    print(e.args[0])

try:
    urllib3.disable_warnings()
    response = get('https://localhost',verify=False)
    print(response.status_code)
except exceptions.SSLError as e:
    print(e.args[0])
```
# 使用代理
+ 描述
requests使用代理发送请求设置proxies参数即可。
+ 代码
``` py
import requests
proxies = {
    'http':'http://144.123.68.152:25653',
    'https':'http://144.123.68.152:25653'
}

r = requests.get('https://www.tmall.com/',proxies=proxies)
print(r.text)
```
**拓展**
1. 当代理需要验证，可以使用类似：http://user:password@host:port这样的语法来设置代理：
``` py
import requests
proxies={
   'http':'http://user:password@133.22.32.33:3424'
}
r=requests.get('https://www.tmall.com',proxies=proxies)
print(r.text)
```
2. Request还支持SOCKS协议代理需要使用第三方库
安装命令如下：
``` shell
pip install requests[socks]
```
配置代理：
``` py
'http':'socks5'://user:pass@host:port',
'https':'socks5'://user:pass@host:port'
```
# 超时
+ 描述
向服务端发送请求分为连接和响应，可以设置连接和响应的时间，用两个元素的元组分别表示连接和读取的超时时间(2,4)，永久等待设置为None。
+ 代码
``` py
import requests,requests.exceptions
try:
    r = requests.get('https://www.jd.com',timeout = 0.001)
    print(r.text)
except requests.exceptions.Timeout as e:
    print(e)

# 抛出连接超时异常
requests.get('https://www.jd.com', timeout=(0.01, 0.001))

# 永久等待，不会抛出超时异常
requests.get('https://www.jd.com', timeout=None)
```
# 身份验证
+ 描述
requests进行身份验证设置auth参数即可，auth参数值是一个HTTPBasicAuth对象
+ 代码
``` py
import requests

from requests.auth import HTTPBasicAuth

r = requests.get('http://localhost:5000',auth=HTTPBasicAuth('bill','1234'))
print(r.status_code)
print(r.text)
```
# 请求打包
+ 描述
requests可以将请求数据进行打包，Request类用于封装请求信息，然后调用Session的prepare_request方法处理Request对象，并返回一个requests.models.Response对象，最后通过Session.send方法发送Response对象。
+ 代码
``` py
from requests import Request,Session

url = 'http://httpbin.org/post'

data = {
    'name':'Bill',
    'age':30
}
headers = {
    'country':'China'
}

session = Session()
req = Request('post',url,data=data,headers=headers)
prepared = session.prepare_request(req)
r = session.send(prepared)
print(type(r))
print(r.text)
```
# python正则表达式使用
## findall方法使用
+ 语法
``` py
import re
re.findall(pattern,string)
```
+ 说明
findall()方法用于查找字符串中符合条件的部分。其有两个参数，第一个参数是正则表达式，第二个是字符串。该方法返回一个列表，里面存放符合条件的子字符串。
+ 例子
``` py
import re
result=re.findall(r"\d{3}-\d{6}","我的电话是：020-225145,140-456147")
print(result)
```
## sub()方法
+ 语法
re.sub(pattern,replace,string,count=n)
+ 说明
参数pattern是一个正则表达式用来匹配符合的子字符串，replace是要替换符合条件字符的字符串，string是要替换的整个字符串，count是替换前n个符合条件的子字符串。
+ 例子
``` py
import re
string="Hello 123 Python 456"
pattern=r"\+"
result=re.sub(pattern,"222",string)
```
匹配结果是：Hello 222 Python 222
## match()和search()方法
+ 语法

``` py
import re
re.match(pattern,string)
re.search(pattern,string)
```
+ 区别
1. match()方法从字符串的首字母开始匹配，如果首字母不符合条件立即返回none。
2. search()方法从左到右搜索整个字符串，然后返回包含第一个匹配结果的对象。
+ 例子
``` py
import re
string="AA11BB"
re.match("\d+",string)
re.search("\d+",string)
print(r1)  //none
print(r2)   //<_sre.SRE_Match object;span(2,4),match="11">
```
**返回第一个匹配项**

``` py
import re
string="11dfsvfsdf"
r1=re.match("\d+",string)
r2=re.search("\d+",string)
print(r1.group()) //11
print(r2.group()) //11
```
> match()和search()方法只会返回符合条件的第一个对象，而findall()方法会返回符合条件的所有对象
{.is-info}

## finditer()方法
+ 描述
该方法和findall方法差不多，只不过它返回的是迭代器
+ 代码
``` py
it=re.finditer("m","mai le fo len,mai ni mei!")
for el in it:
           pring(el.group())
```
## compile()正则预加载
+ 描述
使用该方法可以将长长的正则进行预加载，方便后面使用
+ 代码
``` py
obj=re.compile(r'\d{3}') #将正则表达式编译为一个正则表达式对象
ret=obj.search('abc123455eeeee')
print(ret.group()) #结果：123
```
## 正则分组起别名（正则匹配内容单独提取）
+ 描述
为正则匹配的组起别名可以单独提取需要的内容
+ 代码
``` py
s="""
<div class='西游记'><span id='10010'>中国联通</span></div>
"""
obj=re.compile(r"<span id='(?P<id>\d+)'>(?P<name>\w+)</span>",re.S)
result=obj.search(s)
print(result.group()) #结果：<span id='10010'>中国联通</span>
print(result.group('id')) #结果：10010
print(result.group('name')) #结果：中国联通
```
> findall(),finditer(),search()方法返回结果的介绍
{.is-info}
+ 描述
对于返回结果可以使用group()、groups()、groupdict()方法处理获取的结果，它们的介绍如下：
1. group()
group()或者group(0)用于获取匹配的整个字符串
group(1),group(2)用于取出第一个，第二个匹配的字符串
2. groups()
groups()方法返回一个包含所有参与匹配的子组（不包含组0）的匹配到的搜索文本子串的元祖
3. groupdict()
该方法返回一个包含所有匹配到的命名组的组名为键值和命名组匹配到的搜索文本子串为值作为元素的字典，且groupdict仅能访问命名
组数据

## 实战案列
+ 案列1
``` py
# 拿到页面源代码.   requests
# 通过re来提取想要的有效信息  re
import requests
import re
import csv

url = "https://movie.douban.com/top250"
headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36"
}
resp = requests.get(url, headers=headers)
page_content = resp.text

# 解析数据
obj = re.compile(r'<li>.*?<div class="item">.*?<span class="title">(?P<name>.*?)'
                 r'</span>.*?<p class="">.*?<br>(?P<year>.*?)&nbsp.*?<span '
                 r'class="rating_num" property="v:average">(?P<score>.*?)</span>.*?'
                 r'<span>(?P<num>.*?)人评价</span>', re.S)
# 开始匹配
result = obj.finditer(page_content)
f = open("data.csv", mode="w")
csvwriter = csv.writer(f)
for it in result:
    # print(it.group("name"))
    # print(it.group("score"))
    # print(it.group("num"))
    # print(it.group("year").strip())
    dic = it.groupdict()
    dic['year'] = dic['year'].strip()
    csvwriter.writerow(dic.values())

f.close()
print("over!")
```
+ 案例2
``` py
# 1. 定位到2020必看片
# 2. 从2020必看片中提取到子页面的链接地址
# 3. 请求子页面的链接地址. 拿到我们想要的下载地址....
import requests
import re

domain = "https://www.dytt89.com/"

resp = requests.get(domain, verify=False)  # verify=False 去掉安全验证
resp.encoding = 'gb2312'  # 指定字符集
# print(resp.text)

# 拿到ul里面的li
obj1 = re.compile(r"2020必看热片.*?<ul>(?P<ul>.*?)</ul>", re.S)
obj2 = re.compile(r"<a href='(?P<href>.*?)'", re.S)
obj3 = re.compile(r'◎片　　名(?P<movie>.*?)<br />.*?<td '
                  r'style="WORD-WRAP: break-word" bgcolor="#fdfddf"><a href="(?P<download>.*?)">', re.S)

result1 = obj1.finditer(resp.text)
child_href_list = []
for it in result1:
    ul = it.group('ul')

    # 提取子页面链接:
    result2 = obj2.finditer(ul)
    for itt in result2:
        # 拼接子页面的url地址:  域名 + 子页面地址
        child_href = domain + itt.group('href').strip("/")
        child_href_list.append(child_href)  # 把子页面链接保存起来


# 提取子页面内容
for href in child_href_list:
    child_resp = requests.get(href, verify=False)
    child_resp.encoding = 'gb2312'
    result3 = obj3.search(child_resp.text)
    print(result3.group("movie"))
    print(result3.group("download"))
    # break  # 测试用
```
# Beautiful Soup使用（抓取源码内容）
## 入门使用
+ 安装
``` shell
pip install beautifulsoup4
```
+ 官方文档
 [中文官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)
 + 入门代码
 ``` py
 from bs4 import BeautifulSoup

html = '''
<html>
    <head><title>这是一个演示页面</title></head>
    <body>
        <a href='a.html'>第一页</a>
        <p>
        <a href='b.html'>第二页</a>
    </body>
</html>
'''

soup = BeautifulSoup(html,'lxml')
print('<' + soup.title.string + '>')
print('[' + soup.a["href"]+ ']')
print(soup.prettify())
 ```
>  prettify()方法用于输出经过格式化的html代码
{.is-success}

## 选择节点
+ 获取节点的名称
``` py
soup.title.name
```
+ 获取节点的属性
使用attrs属性可以获取节点所有属性以字典方式返回
``` py
print(soup.li.attrs)
print(soup.li.attrs['value1'])
```
+ 获取节点的内容
通过string属性可以获取节点包含的文本内容
``` py
print(soup.a.string)
```
**案列**
``` py
from bs4 import BeautifulSoup

html = '''
<html>
<head>
    <meta charset="UTF-8">
    <title>Beautiful Soup演示</title>
</head>
<body>
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world"><a href="https://geekori.com"> geekori.com</a></li>
        <li class="item2"><a href="https://www.jd.com"> 京东商城</a></li>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item4" ><a href="https://www.microsoft.com">微软</a></li>
        <li class="item5"><a href="https://www.google.com">谷歌</a></li>
    </ul>
</div>
</body>
</html>
'''
soup = BeautifulSoup(html,'lxml')
print(soup.title.name)
print(soup.li.attrs)
print(soup.li.attrs["value2"])
print(soup.li["value1"])
print(soup.a['href'])
print(soup.a.string)
```
## 嵌套选择节点
+ 描述
获取的节点还可以使用节点选择方法继续获取子节点
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<html>
<head>
    <meta charset="UTF-8">
    <title>Beautiful Soup演示</title>
</head>
<body>
<div>
    <ul>
        <li class="item1"><a href="https://www.jd.com"> 京东商城</a></li>        
    </ul>
</div>
</body>
</html>
'''

soup = BeautifulSoup(html,'lxml')
print(soup.head)
print(type(soup.head))
head = soup.head
print(head.title.string)
print(soup.body.div.ul.li.a['href'])
```
## 选择子节点
1. 获取直接子节点
+ 描述
通过contents和children属性可以获取当前节点的直接子节点，其中contents返回一类列表，children属性返回一个迭代器里面包含直接子节点包括\n
2. 获取所有子孙节点
+ 描述
使用descendants属性可以获取所有子孙节点，该属性返回一个生成器需迭代输出值
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<html>
<head>
    <meta charset="UTF-8">
    <title>Beautiful Soup演示</title>
    <tag1><a><b></b></a></tag1>
</head>
<body>
<div>
    <ul>
        <li class="item1" value = "hello world">
            <a href="https://geekori.com"> 
                geekori.com
            </a>
        </li>
        <li class="item2"><a href="https://www.jd.com"> 京东商城</a></li>

    </ul>
</div>
</body>
</html>
'''

soup = BeautifulSoup(html,'lxml')
print(soup.head.contents)
print(soup.head.children)
print(type(soup.head.contents))
print(type(soup.body.div.ul.children))
print(type(soup.head.descendants))
for i, child in enumerate(soup.body.div.ul.contents):
    print(i,child)
print('----------')
i = 1
for child in soup.body.div.ul.children:
    print('<', i, '>',child, end=" ")
    i += 1
print('----------')
for i, child in enumerate(soup.body.div.ul.descendants):
    print('[',i,']',child)
```
## 选择父节点
+ 描述
使用parent属性可以获取某个节点的直接父节点。parents属性返回当前节点的所有父节点，返回对象可迭代。
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<html>
<head>
    <meta charset="UTF-8">
    <title>Beautiful Soup演示</title>
    <tag1><xyz><b></b></xyz></tag1>
</head>
<body>
<div>
    <ul>
        <li class="item1" value = "hello world">
            <a href="https://geekori.com"> 
                geekori.com
            </a>
        </li>
        <li class="item2"><a href="https://www.jd.com"> 京东商城</a></li>

    </ul>
</div>
</body>
</html>
'''

soup = BeautifulSoup(html,'lxml')
print(soup.a.parent)
print(soup.a.parent['class'])

print(soup.a.parents)
for parent in soup.a.parents:
    print('<',parent.name,'>')
```
## 选择兄弟节点
+ 描述
选择兄弟节点以下属性：
（1）nxet_sibling获取当前节点的下一个兄弟节点
（2）previous_sibling获取当前节点的上一个兄弟节点
（3）next_siblings获取当前节点后面所有兄弟节点
（4）previous_siblings获取当期那节点前所有兄弟节点
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<html>
<head>
    <meta charset="UTF-8">
    <title>Beautiful Soup演示</title>
</head>
<body>
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world">
            <a href="https://geekori.com"> geekori.com</a>
        </li>
        <li class="item2"><a href="https://www.jd.com"> 京东商城</a></li>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item4" ><a href="https://www.microsoft.com">微软</a></li>
        <li class="item5"><a href="https://www.google.com">谷歌</a></li>
    </ul>
</div>
</body>
</html>
'''

soup = BeautifulSoup(html,'lxml')
secondli = soup.li.next_sibling.next_sibling
print('第1个li节点的下一个li节点：',secondli)
print('第2个li节点的上一个li节点的class属性值：', secondli.previous_sibling.previous_sibling['class'])
for sibling in secondli.next_siblings:
    print(type(sibling))
    if str.strip(sibling.string) == "":
        print('换行')
    else:
        print(sibling)
```
## 方法选择器
### fand_all方法
1. name参数
+ 描述
find_all方法会选取所有节点名与name参数值相同的节点
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<html>
<head>
    <meta charset="UTF-8">
    <title>Beautiful Soup演示</title>
</head>
<body>
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world"><a href="https://geekori.com"> geekori.com</a></li>
        <li class="item2"><a href="https://www.jd.com"> 京东商城</a></li>        
    </ul>
    <ul>
    <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item4" ><a href="https://www.microsoft.com">微软</a></li>
        <li class="item5"><a href="https://www.google.com">谷歌</a></li>
    </ul>
</div>
</body>
</html>
'''

soup = BeautifulSoup(html,'lxml')
ulTags = soup.find_all(name='ul')

print(type(ulTags))
for ulTag in ulTags:
    print(ulTag)
print('---------------------')
for ulTag in ulTags:
    liTags = ulTag.find_all(name='li')
    for liTag in liTags:
        print(liTag)
```
2. attrs参数
+ 描述
attrs参数值是一个字典类型，key为节点属性，value为节点属性值
+ 代码
```py
from bs4 import BeautifulSoup

html = '''
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world"><a href="https://geekori.com"> geekori.com</a></li>
        <li class="item"><a href="https://www.jd.com"> 京东商城</a></li>        
    </ul>
    <button id="button1">确定</button>
    <ul>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item" ><a href="https://www.microsoft.com">微软</a></li>
        <li class="item2"><a href="https://www.google.com">谷歌</a></li>
    </ul>
</div>

'''

soup = BeautifulSoup(html,'lxml')
tags = soup.find_all(attrs={"class":"item"})
for tag in tags:
    print(tag)

tags = soup.find_all(class_='item2')
print(tags)
tags = soup.find_all(id='button1')
print(tags)
```
> class为关键字需要使用class_
{.is-info}

3. text参数
+ 描述
通过text参数可以搜索匹配的文本节点，传入的参数可以是字符串也可以是正则表达式
+ 代码
``` py
from bs4 import BeautifulSoup
import re
html = '''
<div>
    <xyz>Hello World, what's this?</xyz>
    <button>Hello, my button. </button>
    <a href='https://geekori.com'>geekori.com</a>
</div>

'''

soup = BeautifulSoup(html,'lxml')
tags = soup.find_all(text='geekori.com')
print(tags)
tags = soup.find_all(text=re.compile('Hello'))
print(tags)
```
### find方法
+ 描述
find方法与find_all方法区别：
（1）find方法查询满足条件的第一个节点
（2）find_all方法返回迭代对象，find方法返回bs4.element.Tag
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world">
              <a href="https://geekori.com"> geekori.com</a>
        </li>
        <li class="item"><a href="https://www.jd.com"> 京东商城</a></li>        
    </ul>
    <ul>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item" ><a href="https://www.microsoft.com">微软</a></li>
        <li class="item2"><a href="https://www.google.com">谷歌</a></li>
    </ul>
</div>

'''

soup = BeautifulSoup(html,'lxml')
tags = soup.find(attrs={"class":"item"})
print(type(tags))
print(tags)
print('----------------')
tags = soup.find_all(attrs={"class":"item"})
print(type(tags))
for tag in tags:
    print(tag)
```
> 除了这find和find_all方法还有很多类似的方法，如下：
{.is-info}

（1）find_parent返回直接父节点
（2）find_parents返回所有祖先节点
（3）find_next_sibling返回后面第一个兄弟节点
（4）find_next_siblings返回后面所有兄弟节点
（5）find_previous_sibling返回前面第一个兄弟节点
（6）find_previous_siblings返回前面所有兄弟节点
（7）find_all_next返回节点后所有符合条件的节点
（8）find_next返回第一个符合条件的节点
（9）find_all_previous返回节点前所有符合条件的节点
（10）find_previous返回节点前第一个符合条件的节点
## css选择器
1. 基本使用
+ 描述
使用select方法可以定位选择器进行节点的查找
+ 代码
``` py
from bs4 import BeautifulSoup

html = '''
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world"><a href="https://geekori.com"> geekori.com</a></li>
        <li class="item"><a href="https://www.jd.com"> 京东商城</a></li>        
    </ul>
    <button id="button1">确定</button>
    <ul>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item" ><a href="https://www.microsoft.com">微软</a></li>
        <li class="item2"><a href="https://www.google.com">谷歌</a></li>
    </ul>
</div>

'''

soup = BeautifulSoup(html,'lxml')
tags = soup.select('.item')
for tag in tags:
    print(tag)

tags = soup.select('#button1')
print(tags)
tags = soup.select('a')[2:]
for tag in tags:
    print(tag)
```
2. 嵌套选择节点
+ 描述
通过选择器查找到的节点同样可以嵌套使用
+ 代码
``` py
from bs4 import BeautifulSoup
html = '''
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world"><a href="https://geekori.com"> geekori.com</a></li>
        <li class="item">
           <a href="https://www.jd.com"> 京东商城</a>
           <a href="https://www.google.com">谷歌</a>
        </li>        
    </ul>
    <ul>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item" ><a href="https://www.microsoft.com">微软</a></li>
    </ul>
</div>

'''

soup = BeautifulSoup(html,'lxml')
tags = soup.select('.item')
print(type(tags))
for tag in tags:
    aTags = tag.select('a')
    for aTag in aTags:
        print(aTag)

print('---------')
for tag in tags:
    aTags = tag.find_all(name='a')
    for aTag in aTags:
        print(aTag)
```
3. 获取属性值与文本
+ 描述
select方法返回tag对象集合，所以使用string属性可获取文本，还可使用get_text方法获取文本，使用attrs同样可以获取属性
+ 代码
``` py
from bs4 import BeautifulSoup
html = '''
<div>
    <ul>
        <li class="item1" value1="1234" value2 = "hello world">
            <a href="https://geekori.com"> geekori.com</a>
        </li>
        <li class="item">
           <a href="https://www.jd.com"> 京东商城</a>
           <a href="https://www.google.com">谷歌</a>
        </li>        
    </ul>
    <ul>
        <li class="item3"><a href="https://www.taobao.com">淘宝</a></li>
        <li class="item" ><a href="https://www.microsoft.com">微软</a></li>
    </ul>
</div>

'''

soup = BeautifulSoup(html,'lxml')
tags = soup.select('.item')
print(type(tags))
for tag in tags:
    aTags = tag.select('a')
    for aTag in aTags:
        print(aTag['href'],aTag.get_text())

print('---------')
for tag in tags:
    aTags = tag.find_all(name='a')
    for aTag in aTags:
        print(aTag.attrs['href'],aTag.string)
```
## 综合案例
``` py
# 1.拿到主页面的源代码. 然后提取到子页面的链接地址, href
# 2.通过href拿到子页面的内容. 从子页面中找到图片的下载地址 img -> src
# 3.下载图片
import requests
from bs4 import BeautifulSoup
import time

url = "https://www.umei.cc/bizhitupian/weimeibizhi/"
resp = requests.get(url)
resp.encoding = 'utf-8'  # 处理乱码

# print(resp.text)
# 把源代码交给bs
main_page = BeautifulSoup(resp.text, "html.parser")
alist = main_page.find("div", class_="TypeList").find_all("a")
# print(alist)
for a in alist:
    href = a.get('href')  # 直接通过get就可以拿到属性的值
    # 拿到子页面的源代码
    child_page_resp = requests.get(href)
    child_page_resp.encoding = 'utf-8'
    child_page_text = child_page_resp.text
    # 从子页面中拿到图片的下载路径
    child_page = BeautifulSoup(child_page_text, "html.parser")
    p = child_page.find("p", align="center")
    img = p.find("img")
    src = img.get("src")
    # 下载图片
    img_resp = requests.get(src)
    # img_resp.content  # 这里拿到的是字节
    img_name = src.split("/")[-1]  # 拿到url中的最后一个/以后的内容
    with open("img/"+img_name, mode="wb") as f:
        f.write(img_resp.content)  # 图片内容写入文件

    print("over!!!", img_name)
    time.sleep(1)
print("all over!!!")
```
# XPath解析
## 环境准备
1. 安装lxml
``` shell
pip install lxml
```
2. 参考文档
XPath最新官方文档：https://www.w3.org/TR/xpath-31
3. 解析html
``` py

```

